{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN not available)\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "require(['codemirror/mode/clike/clike'], function(Clike) { console.log('ROOTaaS - C++ CodeMirror module loaded'); });"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.CodeCell.config_defaults.highlight_modes['magic_text/x-c++src'] = {'reg':[/^%%cpp/]};"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/root_numpy/__init__.py:35: RuntimeWarning: numpy 1.11.1 is currently installed but you installed root_numpy against numpy 1.11.0. Please consider reinstalling root_numpy for this numpy version.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to ROOTaaS 6.06/04\n"
     ]
    }
   ],
   "source": [
    "from nn_packages import *\n",
    "from io_functions import *\n",
    "import numpy as np\n",
    "import root_numpy as rnp\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "#import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation,Input, Dense, Dropout, merge\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadmodel(name, weights = False):\n",
    "    json_file = open('%s.json'%name, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    #load weights into new model\n",
    "    if weights==True:\n",
    "        model.load_weights('%s.h5'%name)\n",
    "    #print (model.summary())\n",
    "    print(\"Loaded model from disk\")\n",
    "    return model\n",
    "\n",
    "def savemodel(model,name=\"neural network\"):\n",
    "\n",
    "    model_name = name\n",
    "    model.summary()\n",
    "    #model.save_weights('%s.h5'%model_name, overwrite=True)\n",
    "    model_json = model.to_json()\n",
    "    with open(\"%s.json\"%model_name, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "        \n",
    "def savelosses(hist, name=\"neural network\"):    \n",
    "    loss = np.array(hist.history['loss'])\n",
    "    valoss = np.array(hist.history['val_loss'])\n",
    "    f = h5py.File(\"%s_losses.h5\"%name,\"w\")\n",
    "    f.create_dataset('loss',data=loss)\n",
    "    f.create_dataset('val_loss',data=valoss)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-keyword arg after keyword arg (<ipython-input-5-36b416ed84eb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-36b416ed84eb>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    loadmodel(name='/jayesh/Notebooks_dev/Models_CSCS/models/bcnn2_regcl',False)\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-keyword arg after keyword arg\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input1 = Input(shape = (1, 20, 20, 25))\n",
    "model1 = Convolution3D(3, 4, 4, 5, input_shape = (1, 20, 20, 25), activation='relu') (input1)\n",
    "model1 = MaxPooling3D()(model1)\n",
    "model1 = Flatten()(model1)\n",
    "\n",
    "input2 = Input(shape = (1, 4, 4, 60))\n",
    "model2 = Convolution3D(10, 2, 2, 6, input_shape = (1, 4, 4, 60), activation='relu')(input2)\n",
    "model2 = MaxPooling3D()(model2)\n",
    "model2 = Flatten()(model2)\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(1000, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "oc = Dense(1,activation='sigmoid', name='label')(bmodel)\n",
    "oe = Dense(1,activation='sigmoid', name='energy')(bmodel)\n",
    "\n",
    "model = Model(input=[input1,input2], output=[oc,oe])\n",
    "model.compile(loss=['binary_crossentropy','mse'], optimizer='sgd')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MixedGen:\n",
    "    #Data generator for regression over energy \n",
    "    def __init__( self, batch_size, filepattern='/data/shared/LCD/EnergyScan_Gamma_Shuffled/GammaEscan_*GeV_fulldataset.h5'):\n",
    "        self.batch_size = batch_size\n",
    "        self.filelist=[]\n",
    "        for i in xrange(1,6):\n",
    "            for j in xrange(1,11):\n",
    "                self.filelist.append('/data/shared/LCD/New_Data/GammaEscan_%d_%d.h5'%(i,j)) \n",
    "        self.train_split = 0.6 \n",
    "        self.test_split = 0.2 \n",
    "        self.validation_split = 0.2\n",
    "        self.fileindex = 0\n",
    "        self.filesize = 0\n",
    "        self.position = 0\n",
    "    #function to call when generating data for training  \n",
    "    def train(self,modeltype=3):\n",
    "        length = len(self.filelist)\n",
    "        #deleting the validation and test set filenames from the filelist\n",
    "        del self.filelist[np.floor(((self.train_split))*length).astype(int):]\n",
    "        return self.batches(modeltype)\n",
    "    #function to call when generating data for testing\n",
    "    \n",
    "    #function to call when generating data for validating\n",
    "    def validation(self,modeltype=3):\n",
    "        length = len(self.filelist)\n",
    "        #modifying the filename list to only include files for validation set\n",
    "        self.filelist = self.filelist[np.floor(self.train_split*length+1).astype(int):np.floor((self.train_split+self.validation_split)*length+1).astype(int)]\n",
    "        return self.batches(modeltype)\n",
    "        \n",
    "    #The function which reads files to gather data until batch size is satisfied\n",
    "    def batch_helper(self, fileindex, position, batch_size):\n",
    "        '''\n",
    "        Yields batches of data of size N\n",
    "        '''\n",
    "        f = h5py.File(self.filelist[fileindex],'r')\n",
    "        self.filesize = np.array(f['ECAL']).shape[0]\n",
    "        #print(self.filelist[fileindex],'first')\n",
    "        if (position + batch_size < self.filesize):\n",
    "            data_ECAL = np.array(f['ECAL'][position : position + batch_size])\n",
    "            data_HCAL = np.array(f['HCAL'][position : position + batch_size])\n",
    "            target = np.array(f['target'][position : position + batch_size][:,:,0:2])\n",
    "            #target = np.delete(target,0,1)\n",
    "\n",
    "            position += batch_size\n",
    "            f.close()\n",
    "            #print('first position',position)\n",
    "            return data_ECAL,data_HCAL, target, fileindex, position\n",
    "        \n",
    "        else:\n",
    "            data_ECAL = np.array(f['ECAL'][position : ])\n",
    "            data_HCAL = np.array(f['HCAL'][position : ])\n",
    "            target = np.array(f['target'][position:][:,:,0:2])\n",
    "            #target = np.delete(target,0,1)\n",
    "            f.close()\n",
    "            \n",
    "            if (fileindex+1 < len(self.filelist)):\n",
    "                if(self.batch_size-data_ECAL.shape[0]>0):\n",
    "                    while(self.batch_size-data_ECAL.shape[0]>0):\n",
    "                        if(int(np.floor((self.batch_size-data_ECAL.shape[0])/self.filesize))==0):\n",
    "                            number_of_files=1\n",
    "                        else:\n",
    "                            number_of_files=int(np.ceil((self.batch_size-data_ECAL.shape[0])/self.filesize))\n",
    "                        for i in xrange(0,number_of_files):\n",
    "                            if(fileindex+i+1>len(self.filelist)):\n",
    "                                fileindex=0\n",
    "                                number_of_files=number_of_files-i\n",
    "                                i=0\n",
    "                            f = h5py.File(self.filelist[fileindex+i+1],'r')\n",
    "                            #print(self.filelist[fileindex+i+1],'second')\n",
    "                            if (self.batch_size-data_ECAL.shape[0]<self.filesize):\n",
    "                                position = self.batch_size-data_ECAL.shape[0]\n",
    "                                data_temp_ECAL = np.array(f['ECAL'][ : position])\n",
    "                                data_temp_HCAL = np.array(f['HCAL'][ : position])\n",
    "                                target_temp = np.array(f['target'][:position][:,:,0:2])\n",
    "                            else:\n",
    "                                data_temp_ECAL = np.array(f['ECAL'])\n",
    "                                data_temp_HCAL = np.array(f['HCAL'])\n",
    "                                target_temp = np.array(f['target'][:,:,0:2])\n",
    "                            f.close()\n",
    "                    #data_, target_, fileindex, position = self.batch_helper(fileindex + 1, 0, batch_size - self.filesize+position)\n",
    "                            #print( data.shape,data_.shape)\n",
    "                            #print( target.shape,target_.shape)\n",
    "                            data_ECAL = np.concatenate((data_ECAL, data_temp_ECAL), axis=0)\n",
    "                            data_HCAL = np.concatenate((data_HCAL, data_temp_HCAL), axis=0)\n",
    "                            target = np.concatenate((target, target_temp), axis=0)\n",
    "                    if (fileindex +i+1<len(self.filelist)):\n",
    "                        fileindex = fileindex +i+1\n",
    "                    else:\n",
    "                        fileindex = 0\n",
    "                else:\n",
    "                    position = 0\n",
    "                    fileindex=fileindex+1\n",
    "            else:\n",
    "                fileindex = 0\n",
    "                position = 0\n",
    "            \n",
    "            return data_ECAL,data_HCAL, target, fileindex, position\n",
    "    #The function which loops indefinitely and continues to return data of the specified batch size\n",
    "    def batches(self, modeltype):\n",
    "        while (self.fileindex < len(self.filelist)):\n",
    "            data_ECAL,data_HCAL, target, self.fileindex, self.position = self.batch_helper(self.fileindex, self.position, self.batch_size)\n",
    "            if data_ECAL.shape[0]!=self.batch_size:\n",
    "                continue\n",
    "            if modeltype==3:\n",
    "                data_ECAL = data_ECAL.reshape((data_ECAL.shape[0],)+(1, 24, 24, 25))\n",
    "                data_HCAL = data_HCAL.reshape((data_HCAL.shape[0],)+(1, 4, 4, 60))\n",
    "                #data = np.swapaxes(data, 1, 3)\n",
    "                #data = np.swapaxes(data, 1, 2)\n",
    "                #data = np.swapaxes(data, 0, 1)\n",
    "                #data=data.reshape((data.shape[0],1,20,20,25))\n",
    "            elif modeltype==2:\n",
    "                data_ECAL = data_ECAL.reshape((data_ECAL.shape[0],)+(24, 24, 25))\n",
    "                data_ECAL = np.swapaxes(data_ECAL, 1, 3)\n",
    "                data_HCAL = data_HCAL.reshape((data_HCAL.shape[0],)+(4, 4, 60))\n",
    "                data_HCAL = np.swapaxes(data_HCAL, 1, 3)\n",
    "            elif modeltype==1:\n",
    "                data_ECAL= np.reshape(data_ECAL,(self.batch_size,-1))\n",
    "                data_HCAL= np.reshape(data_HCAL,(self.batch_size,-1))\n",
    "            yield ([data_ECAL,data_HCAL],[target[:,:,0],target[:,:,1]/500.])\n",
    "        self.fileindex = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegGen:\n",
    "    '''\n",
    "    Data generator class for directory of h5 files\n",
    "    '''\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.filelist=[]\n",
    "        for i in xrange(1,6):\n",
    "            for j in xrange(1,11):\n",
    "                self.filelist.append('/data/shared/LCD/New_Data/GammaEscan_%d_%d.h5'%(i,j)) \n",
    "        self.train_split = 0.6\n",
    "        self.test_split = 0.2\n",
    "        self.validation_split = 0.2\n",
    "        self.fileindex = 0\n",
    "        self.filesize = 0\n",
    "        self.position = 0\n",
    "\n",
    "    def train(self,modeltype=3):\n",
    "        '''\n",
    "        Generate data for training only\n",
    "        '''\n",
    "        length = len(self.filelist)\n",
    "        #deleting the validation and test set filenames from the filelist\n",
    "        del self.filelist[int(np.floor((self.train_split) * length)):]\n",
    "        return self.batches(modeltype)\n",
    "    \n",
    "    def validation(self, modeltype=3):\n",
    "        '''\n",
    "        Generate data for validation only\n",
    "        '''\n",
    "        length = len(self.filelist)\n",
    "        #modifying the filename list to only include files for validation set\n",
    "        self.filelist = self.filelist[int(np.floor(self.train_split*length+1)):\\\n",
    "                                      int(np.floor((self.train_split + self.validation_split) * length+1))]\n",
    "        return self.batches(modeltype)\n",
    "        \n",
    "    #The function which reads files to gather data until batch size is satisfied\n",
    "    def batch_helper(self, fileindex, position, batch_size):\n",
    "        '''\n",
    "        Reads files to gather data until batch size is satisfied, then yeilds\n",
    "        '''\n",
    "        f = h5py.File(self.filelist[fileindex], 'r')\n",
    "        self.filesize = np.array(f['ECAL']).shape[0]\n",
    "        #print(self.filelist[fileindex],'first')\n",
    "        if (position + batch_size < self.filesize):\n",
    "            data_ECAL = np.array(f['ECAL'][position : position + batch_size])\n",
    "            data_HCAL = np.array(f['HCAL'][position : position + batch_size])\n",
    "            target  = np.array(f['target'][position : position + batch_size][:,:,1])\n",
    "            #target = np.delete(target,0,1)\n",
    "\n",
    "            position += batch_size\n",
    "            f.close()\n",
    "            #print('first position',position)\n",
    "            return data_ECAL, data_HCAL, target, fileindex, position\n",
    "        \n",
    "        else:\n",
    "            data_ECAL = np.array(f['ECAL'][position:])\n",
    "            data_HCAL = np.array(f['HCAL'][position:])\n",
    "            target =  np.array(f['target'][position:][:,:,1])\n",
    "            #target = np.delete(target,0,1)\n",
    "            f.close()\n",
    "            \n",
    "            if fileindex+1 < len(self.filelist):\n",
    "                if self.batch_size - data_ECAL.shape[0] > 0:\n",
    "                    while self.batch_size - data_ECAL.shape[0] > 0:\n",
    "\n",
    "                        if int(np.floor((self.batch_size - data_ECAL.shape[0]) / self.filesize)) == 0:\n",
    "                            number_of_files = 1\n",
    "                        else:\n",
    "                            number_of_files = int(np.ceil((self.batch_size-data_ECAL.shape[0]) / self.filesize))\n",
    "\n",
    "                        for i in xrange(0, number_of_files):\n",
    "                            # restart in file list in case we run out of files\n",
    "                            if fileindex + i + 1 > len(self.filelist):\n",
    "                                fileindex = -1 - i\n",
    "\n",
    "                            f = h5py.File(self.filelist[fileindex+i+1],'r')\n",
    "                            #print(self.filelist[fileindex+i+1],'second')\n",
    "                            if (self.batch_size - data_ECAL.shape[0] < self.filesize):\n",
    "                                position = self.batch_size - data_ECAL.shape[0]\n",
    "                                data_temp_ECAL = np.array(f['ECAL'][:position])\n",
    "                                data_temp_HCAL = np.array(f['HCAL'][:position])\n",
    "                                target_temp = np.array(f['target'][:position][:,:,1])\n",
    "\n",
    "                            else:\n",
    "                                data_temp_ECAL = np.array(f['ECAL'])\n",
    "                                data_temp_HCAL = np.array(f['HCAL'])\n",
    "                                target_temp = np.array(f['target'][:,:,1])\n",
    "\n",
    "                            f.close()\n",
    "                            #print( data.shape,data_.shape)\n",
    "                            #print( target.shape,target_.shape)\n",
    "                            data_ECAL = np.concatenate((data_ECAL, data_temp_ECAL), axis=0)\n",
    "                            data_HCAL = np.concatenate((data_HCAL, data_temp_HCAL), axis=0)\n",
    "                            target = np.concatenate((target, target_temp), axis=0)\n",
    "                    \n",
    "                    if (fileindex + i + 1 < len(self.filelist)):\n",
    "                        fileindex += i + 1\n",
    "                    else:\n",
    "                        fileindex = 0\n",
    "                else:\n",
    "                    position = 0\n",
    "                    fileindex += 1\n",
    "            else:\n",
    "                fileindex = 0\n",
    "                position = 0\n",
    "            \n",
    "            return data_ECAL, data_HCAL, target, fileindex, position\n",
    "    \n",
    "    def batches(self, modeltype):\n",
    "        '''\n",
    "        Loops indefinitely and continues to return data of specified batch size\n",
    "        '''\n",
    "        while (self.fileindex < len(self.filelist)):\n",
    "            data_ECAL,data_HCAL, target, self.fileindex, self.position = self.batch_helper(self.fileindex, self.position, self.batch_size)\n",
    "            if data_ECAL.shape[0]!=self.batch_size:\n",
    "                continue\n",
    "            if modeltype==3:\n",
    "                data_ECAL = data_ECAL.reshape((data_ECAL.shape[0],)+(1, 24, 24, 25))\n",
    "                data_HCAL = data_HCAL.reshape((data_HCAL.shape[0],)+(1, 4, 4, 60))\n",
    "                #data = np.swapaxes(data, 1, 3)\n",
    "                #data = np.swapaxes(data, 1, 2)\n",
    "                #data = np.swapaxes(data, 0, 1)\n",
    "                #data=data.reshape((data.shape[0],1,20,20,25))\n",
    "            elif modeltype==2:\n",
    "                data_ECAL = data_ECAL.reshape((data_ECAL.shape[0],)+(24, 24, 25))\n",
    "                data_ECAL = np.swapaxes(data_ECAL, 1, 3)\n",
    "                data_HCAL = data_HCAL.reshape((data_HCAL.shape[0],)+(4, 4, 60))\n",
    "                data_HCAL = np.swapaxes(data_HCAL, 1, 3)\n",
    "            elif modeltype==1:\n",
    "                data_ECAL= np.reshape(data_ECAL,(self.batch_size,-1))\n",
    "                data_HCAL= np.reshape(data_HCAL,(self.batch_size,-1))\n",
    "            yield ([data_ECAL,data_HCAL],target/500.)\n",
    "        self.fileindex = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClsGen:\n",
    "    #Data generator for regression over energy \n",
    "    def __init__( self, batch_size, filepattern='/data/shared/LCD/EnergyScan_Gamma_Shuffled/GammaEscan_*GeV_fulldataset.h5'):\n",
    "        self.batch_size = batch_size\n",
    "        self.filelist=[]\n",
    "        for i in xrange(1,6):\n",
    "            for j in xrange(1,11):\n",
    "                self.filelist.append('/data/shared/LCD/New_Data/GammaEscan_%d_%d.h5'%(i,j)) \n",
    "        self.train_split = 0.6 \n",
    "        self.test_split = 0.2 \n",
    "        self.validation_split = 0.2\n",
    "        self.fileindex = 0\n",
    "        self.filesize = 0\n",
    "        self.position = 0\n",
    "    #function to call when generating data for training  \n",
    "    def train(self,modeltype=3):\n",
    "        length = len(self.filelist)\n",
    "        #deleting the validation and test set filenames from the filelist\n",
    "        del self.filelist[np.floor(((self.train_split))*length).astype(int):]\n",
    "        return self.batches(modeltype)\n",
    "    #function to call when generating data for testing\n",
    "   \n",
    "    #function to call when generating data for validating\n",
    "    def validation(self,modeltype=3):\n",
    "        length = len(self.filelist)\n",
    "        #modifying the filename list to only include files for validation set\n",
    "        self.filelist = self.filelist[np.floor(self.train_split*length+1).astype(int):np.floor((self.train_split+self.validation_split)*length+1).astype(int)]\n",
    "        return self.batches(modeltype)\n",
    "        \n",
    "    #The function which reads files to gather data until batch size is satisfied\n",
    "    def batch_helper(self, fileindex, position, batch_size):\n",
    "        '''\n",
    "        Yields batches of data of size N\n",
    "        '''\n",
    "        f = h5py.File(self.filelist[fileindex],'r')\n",
    "        self.filesize = np.array(f['ECAL']).shape[0]\n",
    "        #print(self.filelist[fileindex],'first')\n",
    "        if (position + batch_size < self.filesize):\n",
    "            data_ECAL = np.array(f['ECAL'][position : position + batch_size])\n",
    "            data_HCAL = np.array(f['HCAL'][position : position + batch_size])\n",
    "            target = np.array(f['target'][position : position + batch_size][:,:,0])\n",
    "            #target = np.delete(target,0,1)\n",
    "\n",
    "            position += batch_size\n",
    "            f.close()\n",
    "            #print('first position',position)\n",
    "            return data_ECAL,data_HCAL, target, fileindex, position\n",
    "        \n",
    "        else:\n",
    "            data_ECAL = np.array(f['ECAL'][position : ])\n",
    "            data_HCAL = np.array(f['HCAL'][position : ])\n",
    "            target = np.array(f['target'][position:][:,:,0])\n",
    "            #target = np.delete(target,0,1)\n",
    "            f.close()\n",
    "            \n",
    "            if (fileindex+1 < len(self.filelist)):\n",
    "                if(self.batch_size-data_ECAL.shape[0]>0):\n",
    "                    while(self.batch_size-data_ECAL.shape[0]>0):\n",
    "                        if(int(np.floor((self.batch_size-data_ECAL.shape[0])/self.filesize))==0):\n",
    "                            number_of_files=1\n",
    "                        else:\n",
    "                            number_of_files=int(np.ceil((self.batch_size-data_ECAL.shape[0])/self.filesize))\n",
    "                        for i in xrange(0,number_of_files):\n",
    "                            if(fileindex+i+1>len(self.filelist)):\n",
    "                                fileindex=0\n",
    "                                number_of_files=number_of_files-i\n",
    "                                i=0\n",
    "                            f = h5py.File(self.filelist[fileindex+i+1],'r')\n",
    "                            #print(self.filelist[fileindex+i+1],'second')\n",
    "                            if (self.batch_size-data_ECAL.shape[0]<self.filesize):\n",
    "                                position = self.batch_size-data_ECAL.shape[0]\n",
    "                                data_temp_ECAL = np.array(f['ECAL'][ : position])\n",
    "                                data_temp_HCAL = np.array(f['HCAL'][ : position])\n",
    "                                target_temp = np.array(f['target'][:position][:,:,0])\n",
    "                            else:\n",
    "                                data_temp_ECAL = np.array(f['ECAL'])\n",
    "                                data_temp_HCAL = np.array(f['HCAL'])\n",
    "                                target_temp = np.array(f['target'][:,:,0])\n",
    "                            f.close()\n",
    "                    #data_, target_, fileindex, position = self.batch_helper(fileindex + 1, 0, batch_size - self.filesize+position)\n",
    "                            #print( data.shape,data_.shape)\n",
    "                            #print( target.shape,target_.shape)\n",
    "                            data_ECAL = np.concatenate((data_ECAL, data_temp_ECAL), axis=0)\n",
    "                            data_HCAL = np.concatenate((data_HCAL, data_temp_HCAL), axis=0)\n",
    "                            target = np.concatenate((target, target_temp), axis=0)\n",
    "                    if (fileindex +i+1<len(self.filelist)):\n",
    "                        fileindex = fileindex +i+1\n",
    "                    else:\n",
    "                        fileindex = 0\n",
    "                else:\n",
    "                    position = 0\n",
    "                    fileindex=fileindex+1\n",
    "            else:\n",
    "                fileindex = 0\n",
    "                position = 0\n",
    "            \n",
    "            return data_ECAL,data_HCAL, target, fileindex, position\n",
    "    #The function which loops indefinitely and continues to return data of the specified batch size\n",
    "    def batches(self, modeltype):\n",
    "        while (self.fileindex < len(self.filelist)):\n",
    "            data_ECAL,data_HCAL, target, self.fileindex, self.position = self.batch_helper(self.fileindex, self.position, self.batch_size)\n",
    "            if data_ECAL.shape[0]!=self.batch_size:\n",
    "                continue\n",
    "            if modeltype==3:\n",
    "                data_ECAL = data_ECAL.reshape((data_ECAL.shape[0],)+(1, 24, 24, 25))\n",
    "                data_HCAL = data_HCAL.reshape((data_HCAL.shape[0],)+(1, 4, 4, 60))\n",
    "                #data = np.swapaxes(data, 1, 3)\n",
    "                #data = np.swapaxes(data, 1, 2)\n",
    "                #data = np.swapaxes(data, 0, 1)\n",
    "                #data=data.reshape((data.shape[0],1,20,20,25))\n",
    "            elif modeltype==2:\n",
    "                data_ECAL = data_ECAL.reshape((data_ECAL.shape[0],)+(24, 24, 25))\n",
    "                data_ECAL = np.swapaxes(data_ECAL, 1, 3)\n",
    "                data_HCAL = data_HCAL.reshape((data_HCAL.shape[0],)+(4, 4, 60))\n",
    "                data_HCAL = np.swapaxes(data_HCAL, 1, 3)\n",
    "            elif modeltype==1:\n",
    "                data_ECAL= np.reshape(data_ECAL,(self.batch_size,-1))\n",
    "                data_HCAL= np.reshape(data_HCAL,(self.batch_size,-1))\n",
    "            yield ([data_ECAL,data_HCAL],target)\n",
    "        self.fileindex = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 1, 24, 24, 25) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 1, 4, 4, 60)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_1 (Convolution3D)  (None, 10, 21, 21, 21)810         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_2 (Convolution3D)  (None, 10, 3, 3, 55)  250         input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling3d_1 (MaxPooling3D)    (None, 10, 10, 10, 10)0           convolution3d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling3d_2 (MaxPooling3D)    (None, 10, 1, 1, 27)  0           convolution3d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 10000)         0           maxpooling3d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 270)           0           maxpooling3d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 10270)         0           flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 10000)         102710000   merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 10000)         0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 100)           1000100     dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 100)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 10)            1010        dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 10)            0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "energy (Dense)                   (None, 1)             11          dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 103712181\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = loadmodel('Models_CSCS/bcnn1_reg')\n",
    "model.compile(loss=['mse'],optimizer='sgd')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 67s - loss: 0.1073 - val_loss: 0.0917\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 59s - loss: 0.1063 - val_loss: 0.0960\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 52s - loss: 0.1003 - val_loss: 0.0879\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 30s - loss: 0.1055 - val_loss: 0.0863\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 30s - loss: 0.1001 - val_loss: 0.0844\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 30s - loss: 0.1023 - val_loss: 0.0895\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 31s - loss: 0.1029 - val_loss: 0.0875\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 30s - loss: 0.1041 - val_loss: 0.0838\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 31s - loss: 0.0930 - val_loss: 0.0877\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 31s - loss: 0.0972 - val_loss: 0.0876\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 34s - loss: 0.1009 - val_loss: 0.0830\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 37s - loss: 0.0959 - val_loss: 0.0845\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 38s - loss: 0.0961 - val_loss: 0.0878\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 33s - loss: 0.1003 - val_loss: 0.0854\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 32s - loss: 0.1012 - val_loss: 0.0774\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 33s - loss: 0.0878 - val_loss: 0.0805\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 33s - loss: 0.0934 - val_loss: 0.0833\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 32s - loss: 0.0949 - val_loss: 0.0785\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 33s - loss: 0.0931 - val_loss: 0.0789\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 32s - loss: 0.0926 - val_loss: 0.0779\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 33s - loss: 0.0922 - val_loss: 0.0755\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 34s - loss: 0.0952 - val_loss: 0.0802\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 41s - loss: 0.0890 - val_loss: 0.0755\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 33s - loss: 0.0902 - val_loss: 0.0784\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 33s - loss: 0.0935 - val_loss: 0.0785\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 47s - loss: 0.0920 - val_loss: 0.0807\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 34s - loss: 0.0905 - val_loss: 0.0737\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 34s - loss: 0.0985 - val_loss: 0.0780\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 34s - loss: 0.0916 - val_loss: 0.0773\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 34s - loss: 0.0888 - val_loss: 0.0756\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 35s - loss: 0.0919 - val_loss: 0.0727\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 37s - loss: 0.0889 - val_loss: 0.0726\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 49s - loss: 0.0880 - val_loss: 0.0719\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 34s - loss: 0.0903 - val_loss: 0.0751\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 35s - loss: 0.0889 - val_loss: 0.0739\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 34s - loss: 0.0884 - val_loss: 0.0741\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 46s - loss: 0.0850 - val_loss: 0.0696\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 37s - loss: 0.0837 - val_loss: 0.0733\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 34s - loss: 0.0855 - val_loss: 0.0753\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 32s - loss: 0.0847 - val_loss: 0.0763\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 33s - loss: 0.0853 - val_loss: 0.0699\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 35s - loss: 0.0895 - val_loss: 0.0709\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 41s - loss: 0.0814 - val_loss: 0.0676\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 34s - loss: 0.0854 - val_loss: 0.0687\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 34s - loss: 0.0848 - val_loss: 0.0702\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 36s - loss: 0.0861 - val_loss: 0.0708\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 35s - loss: 0.0822 - val_loss: 0.0715\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 34s - loss: 0.0858 - val_loss: 0.0684\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 32s - loss: 0.0898 - val_loss: 0.0777\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 32s - loss: 0.0808 - val_loss: 0.0694\n"
     ]
    }
   ],
   "source": [
    "ds1 = RegGen(500)\n",
    "vs1 = RegGen(500)\n",
    "#check = ModelCheckpoint(filepath=\"temp.hdf5\", verbose=1)\n",
    "#early = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "hist = model.fit_generator(ds1.train(modeltype=3), samples_per_epoch=1000, nb_epoch=50, validation_data= vs1.validation(modeltype=3), nb_val_samples=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dense Model\n",
    "input1 = Input(shape = (14400,))\n",
    "model1 = Dense(10000, activation='relu') (input1)\n",
    "model1 = Dense(10000, activation='relu') (model1)\n",
    "\n",
    "input2 = Input(shape = (960,))\n",
    "model2 = Dense(10000,activation='relu')(input2)\n",
    "model2 = Dense(10000, activation='relu')(model2)\n",
    "\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='sigmoid')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='sigmoid')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='sigmoid')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "oc = Dense(1,activation='sigmoid', name='particle label')(bmodel)\n",
    "oe = Dense(1,activation='sigmoid', name='energy')(bmodel)\n",
    "\n",
    "model = Model(input=[input1,input2], output=[oc,oe])\n",
    "model.compile(loss=['binary_crossentropy','mse'], optimizer='sgd')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Regression Model\n",
    "input1 = Input(shape = (25,24,24))\n",
    "model1 = Convolution2D(10,4, 4,input_shape = (25,24,24), activation='relu') (input1)\n",
    "model1 = Convolution2D(10,3, 3, activation='relu') (model1)\n",
    "model1 = MaxPooling2D()(model1)\n",
    "model1 = Flatten()(model1)\n",
    "\n",
    "input2 = Input(shape = (60,4,4))\n",
    "model2 = Convolution2D(10, 2, 2,input_shape = (60,4,4), activation='relu')(input2)\n",
    "model2 = Convolution2D(3, 2, 2, activation='relu')(model2)\n",
    "model2 = MaxPooling2D()(model2)\n",
    "model2 = Flatten()(model2)\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='sigmoid')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='sigmoid')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "#bmodel = (Dense(10, activation='sigmoid')) (bmodel)\n",
    "#bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "#oc = Dense(1,activation='sigmoid', name='particleID')(bmodel)\n",
    "oe = Dense(1,activation='sigmoid', name='energy')(bmodel)\n",
    "\n",
    "model = Model(input=[input1,input2], output=[oe])\n",
    "model.compile(loss=['mse'], optimizer='sgd')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savemodel(model,'dense1_regcl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds1 = RegGenFull(5000, 10000)\n",
    "vs1 = RegGenFull(5000, 10000)\n",
    "early = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "hist = model.fit_generator(ds1.train(cnn=True), samples_per_epoch=50000, nb_epoch=30, validation_data= vs1.validation(cnn=True), nb_val_samples=50000, verbose=1, callbacks=[early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savemodel(model,'bcnn_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savelosses(hist,'bcnn_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#savemodel(cnn2d_1,\"cnn2d_1\")\n",
    "%matplotlib inline\n",
    "show_losses([(\"mse\",hist)],\"bcnn_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(filename):\n",
    "    f = h5py.File(filename,'r')\n",
    "    HCAL = np.array(f['HCAL'])\n",
    "    ECAL = np.array(f['ECAL'])\n",
    "    target=np.array(f['target'])\n",
    "    f.close()\n",
    "    np.savez_compressed(filename.replace('.h5','.npz'), HCAL=HCAL,ECAL=ECAL,target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(1,101):\n",
    "    print(i)\n",
    "    convert('/data/shared/LCD/New_Data_Shuffled/GammaPi0_shuffled_%d.h5'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_file = open('bcnn_reg_4.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_file = open('models/bcnn_cl_1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = loadmodel('models/bcnn_cl_1',weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "#import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation,Input, Dense, Dropout, merge\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load the models' predictions and histories\n",
    "#predictions = {}\n",
    "#histories = {}\n",
    "model_names = []\n",
    "mc = 0\n",
    "for l in filter(None, os.popen('ls models/*.json').read().split('\\n')):\n",
    "    #m_name = l.split('/')[-1].split('.')[0]\n",
    "    l = l.split('.')[0]\n",
    "    print (mc+1, l)\n",
    "    mc = mc+1 \n",
    "    model_names.append(l)   \n",
    "    # Load model information\n",
    "    model = loadmodel(l, weights=False)\n",
    "    print (m_name)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'bcnn_1_reg.json'\n",
    "print(filename.replace('.json','')+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'Models_CSCS/dense1_cl'\n",
    "ds = ClsGen(500)\n",
    "vs = ClsGen(500)\n",
    "model = loadmodel(filename)\n",
    "model.compile(loss=['binary_crossentropy'],optimizer='sgd')\n",
    "check = ModelCheckpoint(filepath=\"temp.h5\", verbose=0)\n",
    "early = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
    "print('%s has started training'%filename)\n",
    "hist = model.fit_generator(ds.train(modeltype=1), samples_per_epoch=500, nb_epoch=1, validation_data= vs.validation(modeltype=1), nb_val_samples=500, verbose=0, callbacks=[check,early])\n",
    "savelosses(hist,name=filename.replace('.json',''))\n",
    "savemodel(model,name=filename.replace('.json',''))\n",
    "print('%s has finished training'%filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelsave(model,name=\"neural network\"):\n",
    "\n",
    "    model_name = name\n",
    "    print name\n",
    "    model.summary()\n",
    "    #model.save_weights('%s.h5'%model_name, overwrite=True)\n",
    "    model_json = model.to_json()\n",
    "    with open('models/'+\"%s.json\"%model_name, \"w\") as json_file:\n",
    "        json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All same topologies as above, but with relu instead of sigmoid\n",
    "\n",
    "#Regression Models \n",
    "\n",
    "##Model 1\n",
    "input1 = Input(shape = (1, 24, 24, 25))\n",
    "model1 = Convolution3D(10, 4, 4, 5, input_shape = (1, 24, 24, 25), activation='relu') (input1)\n",
    "model1 = MaxPooling3D()(model1)\n",
    "model1 = Flatten()(model1)\n",
    "\n",
    "input2 = Input(shape = (1, 4, 4, 60))\n",
    "model2 = Convolution3D(10, 2, 2, 6, input_shape = (1, 4, 4, 60), activation='relu')(input2)\n",
    "model2 = MaxPooling3D()(model2)\n",
    "model2 = Flatten()(model2)\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(10, activation='relu')) (bmodel)\n",
    "\n",
    "#oc = Dense(1,activation='sigmoid', name='particle label')(bmodel)\n",
    "oe = Dense(1,activation='linear', name='energy')(bmodel)\n",
    "\n",
    "bcnnr1 = Model(input=[input1,input2], output=[oe])\n",
    "bcnnr1.compile(loss=['mse'], optimizer='sgd')\n",
    "bcnnr1.summary()\n",
    "\n",
    "\n",
    "\n",
    "#Regression Models \n",
    "\n",
    "##Model 2\n",
    "input1 = Input(shape = (1, 24, 24, 25))\n",
    "model1 = Convolution3D(10, 4, 4, 5, input_shape = (1, 24, 24, 25), activation='relu') (input1)\n",
    "model1 = MaxPooling3D()(model1)\n",
    "model1 = Flatten()(model1)\n",
    "\n",
    "input2 = Input(shape = (1, 4, 4, 60))\n",
    "model2 = Convolution3D(10, 2, 2, 6, input_shape = (1, 4, 4, 60), activation='relu')(input2)\n",
    "model2 = MaxPooling3D()(model2)\n",
    "model2 = Flatten()(model2)\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "#oc = Dense(1,activation='sigmoid', name='particle label')(bmodel)\n",
    "oe = Dense(1,activation='linear', name='energy')(bmodel)\n",
    "\n",
    "bcnnr2 = Model(input=[input1,input2], output=[oe])\n",
    "bcnnr2.compile(loss=['mse'], optimizer='sgd')\n",
    "bcnnr2.summary()\n",
    "\n",
    "\n",
    "#Regression Models \n",
    "\n",
    "##Model 3\n",
    "input1 = Input(shape = (1, 24, 24, 25))\n",
    "model1 = Convolution3D(10, 4, 4, 5, input_shape = (1, 24, 24, 25), activation='relu') (input1)\n",
    "model1 = MaxPooling3D()(model1)\n",
    "model1 = Flatten()(model1)\n",
    "\n",
    "input2 = Input(shape = (1, 4, 4, 60))\n",
    "model2 = Convolution3D(10, 2, 2, 6, input_shape = (1, 4, 4, 60), activation='relu')(input2)\n",
    "model2 = MaxPooling3D()(model2)\n",
    "model2 = Flatten()(model2)\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "#oc = Dense(1,activation='sigmoid', name='particle label')(bmodel)\n",
    "oe = Dense(1,activation='linear', name='energy')(bmodel)\n",
    "\n",
    "bcnnr3 = Model(input=[input1,input2], output=[oe])\n",
    "bcnnr3.compile(loss=['mse'], optimizer='sgd')\n",
    "bcnnr3.summary()\n",
    "\n",
    "#Regression Models \n",
    "\n",
    "##Model 4\n",
    "input1 = Input(shape = (1, 24, 24, 25))\n",
    "model1 = Convolution3D(10, 4, 4, 5, input_shape = (1, 24, 24, 25), activation='relu') (input1)\n",
    "model1 = Convolution3D(3, 2, 2, 5, input_shape = (1, 24, 24, 25), activation='relu') (model1)\n",
    "model1 = MaxPooling3D()(model1)\n",
    "model1 = Flatten()(model1)\n",
    "\n",
    "input2 = Input(shape = (1, 4, 4, 60))\n",
    "model2 = Convolution3D(10, 2, 2, 6, input_shape = (1, 4, 4, 60), activation='relu')(input2)\n",
    "model2 = Convolution3D(3, 2, 2, 5, input_shape = (1, 4, 4, 60), activation='relu')(model2)\n",
    "model2 = MaxPooling3D()(model2)\n",
    "model2 = Flatten()(model2)\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(10, activation='relu')) (bmodel)\n",
    "\n",
    "#oc = Dense(1,activation='sigmoid', name='particle label')(bmodel)\n",
    "oe = Dense(1,activation='linear', name='energy')(bmodel)\n",
    "\n",
    "bscnnr1 = Model(input=[input1,input2], output=[oe])\n",
    "bscnnr1.compile(loss=['mse'], optimizer='sgd')\n",
    "bscnnr1.summary()\n",
    "\n",
    "#Regression Models \n",
    "\n",
    "##Model 5\n",
    "input1 = Input(shape = (25,24,24))\n",
    "model1 = Convolution2D(10, 4, 4, input_shape = (25,24,24), activation='relu') (input1)\n",
    "model1 = Convolution2D(3, 2, 2,activation='relu') (model1)\n",
    "model1 = MaxPooling2D()(model1)\n",
    "model1 = Flatten()(model1)\n",
    "\n",
    "input2 = Input(shape = (60,4,4))\n",
    "model2 = Convolution2D(10, 2, 2,input_shape = (60,4,4), activation='relu')(input2)\n",
    "model2 = Convolution2D(3, 2, 2,activation='relu')(model2)\n",
    "model2 = MaxPooling2D()(model2)\n",
    "model2 = Flatten()(model2)\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(10, activation='relu')) (bmodel)\n",
    "\n",
    "#oc = Dense(1,activation='sigmoid', name='particle label')(bmodel)\n",
    "oe = Dense(1,activation='linear', name='energy')(bmodel)\n",
    "\n",
    "bscnnr2 = Model(input=[input1,input2], output=[oe])\n",
    "bscnnr2.compile(loss=['mse'], optimizer='sgd')\n",
    "bscnnr2.summary()\n",
    "\n",
    "\n",
    "#Dense Model\n",
    "input1 = Input(shape = (14400,))\n",
    "model1 = Dense(10000, activation='relu') (input1)\n",
    "model1 = Dense(10000, activation='relu') (model1)\n",
    "\n",
    "input2 = Input(shape = (960,))\n",
    "model2 = Dense(10000,activation='relu')(input2)\n",
    "model2 = Dense(10000, activation='relu')(model2)\n",
    "\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "#oc = Dense(1,activation='sigmoid', name='particle label')(bmodel)\n",
    "oe = Dense(1,activation='linear', name='energy')(bmodel)\n",
    "\n",
    "denser = Model(input=[input1,input2], output=[oe])\n",
    "denser.compile(loss=['mse'], optimizer='sgd')\n",
    "denser.summary()\n",
    "\n",
    "\n",
    "modelsave(bscnnr1,\"bcnnr4_reg\")\n",
    "modelsave(bscnnr2,\"bcnnr5_reg\")\n",
    "modelsave(bcnnr1,\"bcnnr1_regc\")\n",
    "modelsave(bcnnr2,\"bcnnr2_reg\")\n",
    "modelsave(bcnnr3,\"bcnnr3_reg\")\n",
    "#draw_model(bcnnr3, \"bcnnr5_reg\")\n",
    "modelsave(denser,'denser1_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All same topologies as above, but with relu instead of sigmoid\n",
    "\n",
    "#Classification Models \n",
    "\n",
    "##Model 1\n",
    "input1 = Input(shape = (1, 24, 24, 25))\n",
    "model1 = Convolution3D(10, 4, 4, 5, input_shape = (1, 24, 24, 25), activation='relu') (input1)\n",
    "model1 = MaxPooling3D()(model1)\n",
    "model1 = Flatten()(model1)\n",
    "\n",
    "input2 = Input(shape = (1, 4, 4, 60))\n",
    "model2 = Convolution3D(10, 2, 2, 6, input_shape = (1, 4, 4, 60), activation='relu')(input2)\n",
    "model2 = MaxPooling3D()(model2)\n",
    "model2 = Flatten()(model2)\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(10, activation='relu')) (bmodel)\n",
    "\n",
    "oc = Dense(1,activation='sigmoid', name='particle label')(bmodel)\n",
    "#oe = Dense(1,activation='linear', name='energy')(bmodel)\n",
    "\n",
    "bcnnr1 = Model(input=[input1,input2], output=[oc])\n",
    "bcnnr1.compile(loss=['binary_crossentropy'], optimizer='sgd')\n",
    "bcnnr1.summary()\n",
    "\n",
    "\n",
    "\n",
    "#Classification Models \n",
    "\n",
    "##Model 2\n",
    "input1 = Input(shape = (1, 24, 24, 25))\n",
    "model1 = Convolution3D(10, 4, 4, 5, input_shape = (1, 24, 24, 25), activation='relu') (input1)\n",
    "model1 = MaxPooling3D()(model1)\n",
    "model1 = Flatten()(model1)\n",
    "\n",
    "input2 = Input(shape = (1, 4, 4, 60))\n",
    "model2 = Convolution3D(10, 2, 2, 6, input_shape = (1, 4, 4, 60), activation='relu')(input2)\n",
    "model2 = MaxPooling3D()(model2)\n",
    "model2 = Flatten()(model2)\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "oc = Dense(1,activation='sigmoid', name='particle label')(bmodel)\n",
    "#oe = Dense(1,activation='linear', name='energy')(bmodel)\n",
    "\n",
    "bcnnr2 = Model(input=[input1,input2], output=[oc])\n",
    "bcnnr2.compile(loss=['binary_crossentropy'], optimizer='sgd')\n",
    "bcnnr2.summary()\n",
    "\n",
    "\n",
    "#Classification Models \n",
    "\n",
    "##Model 3\n",
    "input1 = Input(shape = (1, 24, 24, 25))\n",
    "model1 = Convolution3D(10, 4, 4, 5, input_shape = (1, 24, 24, 25), activation='relu') (input1)\n",
    "model1 = MaxPooling3D()(model1)\n",
    "model1 = Flatten()(model1)\n",
    "\n",
    "input2 = Input(shape = (1, 4, 4, 60))\n",
    "model2 = Convolution3D(10, 2, 2, 6, input_shape = (1, 4, 4, 60), activation='relu')(input2)\n",
    "model2 = MaxPooling3D()(model2)\n",
    "model2 = Flatten()(model2)\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "oc = Dense(1,activation='sigmoid', name='particle label')(bmodel)\n",
    "#oe = Dense(1,activation='linear', name='energy')(bmodel)\n",
    "\n",
    "bcnnr3 = Model(input=[input1,input2], output=[oc])\n",
    "bcnnr3.compile(loss=['binary_crossentropy'], optimizer='sgd')\n",
    "bcnnr3.summary()\n",
    "\n",
    "#Classification Models \n",
    "\n",
    "##Model 4\n",
    "input1 = Input(shape = (1, 24, 24, 25))\n",
    "model1 = Convolution3D(10, 4, 4, 5, input_shape = (1, 24, 24, 25), activation='relu') (input1)\n",
    "model1 = Convolution3D(3, 2, 2, 5, input_shape = (1, 24, 24, 25), activation='relu') (model1)\n",
    "model1 = MaxPooling3D()(model1)\n",
    "model1 = Flatten()(model1)\n",
    "\n",
    "input2 = Input(shape = (1, 4, 4, 60))\n",
    "model2 = Convolution3D(10, 2, 2, 6, input_shape = (1, 4, 4, 60), activation='relu')(input2)\n",
    "model2 = Convolution3D(3, 2, 2, 5, input_shape = (1, 4, 4, 60), activation='relu')(model2)\n",
    "model2 = MaxPooling3D()(model2)\n",
    "model2 = Flatten()(model2)\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(10, activation='relu')) (bmodel)\n",
    "\n",
    "oc = Dense(1,activation='sigmoid', name='particle label')(bmodel)\n",
    "#oe = Dense(1,activation='linear', name='energy')(bmodel)\n",
    "\n",
    "bscnnr1 = Model(input=[input1,input2], output=[oc])\n",
    "bscnnr1.compile(loss=['binary_crossentropy'], optimizer='sgd')\n",
    "bscnnr1.summary()\n",
    "\n",
    "#Classification Models \n",
    "\n",
    "##Model 5\n",
    "input1 = Input(shape = (25,24,24))\n",
    "model1 = Convolution2D(10, 4, 4, input_shape = (25,24,24), activation='relu') (input1)\n",
    "model1 = Convolution2D(3, 2, 2,activation='relu') (model1)\n",
    "model1 = MaxPooling2D()(model1)\n",
    "model1 = Flatten()(model1)\n",
    "\n",
    "input2 = Input(shape = (60,4,4))\n",
    "model2 = Convolution2D(10, 2, 2,input_shape = (60,4,4), activation='relu')(input2)\n",
    "model2 = Convolution2D(3, 2, 2,activation='relu')(model2)\n",
    "model2 = MaxPooling2D()(model2)\n",
    "model2 = Flatten()(model2)\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(10, activation='relu')) (bmodel)\n",
    "\n",
    "oc = Dense(1,activation='sigmoid', name='particle label')(bmodel)\n",
    "#oe = Dense(1,activation='linear', name='energy')(bmodel)\n",
    "\n",
    "bscnnr2 = Model(input=[input1,input2], output=[oc])\n",
    "bscnnr2.compile(loss=['binary_crossentropy'], optimizer='sgd')\n",
    "bscnnr2.summary()\n",
    "\n",
    "\n",
    "#Dense Model\n",
    "input1 = Input(shape = (14400,))\n",
    "model1 = Dense(10000, activation='relu') (input1)\n",
    "model1 = Dense(10000, activation='relu') (model1)\n",
    "\n",
    "input2 = Input(shape = (960,))\n",
    "model2 = Dense(10000,activation='relu')(input2)\n",
    "model2 = Dense(10000, activation='relu')(model2)\n",
    "\n",
    "\n",
    "## join the two\n",
    "bmodel = merge([model1,model2], mode='concat')\n",
    "\n",
    "## fully connected ending\n",
    "bmodel = (Dense(10000, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "bmodel = (Dense(100, activation='relu')) (bmodel)\n",
    "bmodel = (Dropout(0.5)) (bmodel)\n",
    "\n",
    "oc = Dense(1,activation='sigmoid', name='particle label')(bmodel)\n",
    "#oe = Dense(1,activation='linear', name='energy')(bmodel)\n",
    "\n",
    "denser = Model(input=[input1,input2], output=[oc])\n",
    "denser.compile(loss=['binary_crossentropy'], optimizer='sgd')\n",
    "denser.summary()\n",
    "\n",
    "\n",
    "modelsave(bscnnr1,\"bcnnr4_cl\")\n",
    "modelsave(bscnnr2,\"bcnnr5_cl\")\n",
    "modelsave(bcnnr1,\"bcnnr1_cl\")\n",
    "modelsave(bcnnr2,\"bcnnr2_cl\")\n",
    "modelsave(bcnnr3,\"bcnnr3_cl\")\n",
    "#draw_model(bcnnr3, \"bcnnr5_regcl\")\n",
    "modelsave(denser,'denser1_cl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
