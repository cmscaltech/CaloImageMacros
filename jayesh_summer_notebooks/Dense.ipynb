{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook with only dense models being used to run regression on LCD Gamma datasets in energy range 10 - 109 GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN not available)\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "require(['codemirror/mode/clike/clike'], function(Clike) { console.log('ROOTaaS - C++ CodeMirror module loaded'); });"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.CodeCell.config_defaults.highlight_modes['magic_text/x-c++src'] = {'reg':[/^%%cpp/]};"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to ROOTaaS 6.06/04\n"
     ]
    }
   ],
   "source": [
    "from nn_packages import *\n",
    "from io_functions import *\n",
    "import numpy as np\n",
    "import root_numpy as rnp\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "#import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir='/home/kaustuv1993/Notebooks/models/'\n",
    "for file in os.listdir(dir):\n",
    "    if file.startswith('dense'):\n",
    "        print file\n",
    "        json_file = open('%s%s' % (dir,file), 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "        print (model.summary())\n",
    "        print ('+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                    (None, 10000)       100010000   dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                    (None, 100)         1000100     dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                    (None, 10)          1010        dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                    (None, 1)           11          dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 101011121\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnn = Sequential()\n",
    "dnn.add(Dense(10000,input_shape=(10000,), activation='sigmoid'))\n",
    "dnn.add(Dense(100, activation='sigmoid'))\n",
    "#dnn.add(Dropout(0.5))\n",
    "dnn.add(Dense(10, activation='sigmoid'))\n",
    "dnn.add(Dense(1, activation='linear'))\n",
    "#sgd=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=True)\n",
    "dnn.compile(loss='mse', optimizer='sgd')\n",
    "#simple.load_weights('first_try.h5')\n",
    "dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class My_Gen_Reg:\n",
    "    #Data generator for regression over energy \n",
    "    def __init__( self, batch_size, filesize, filepattern='/data/shared/LCD/EnergyScan_Gamma_Shuffled/GammaEscan_*GeV_fulldataset.h5'):\n",
    "        self.batch_size = batch_size\n",
    "        self.filelist=[]\n",
    "        for i in xrange(1,11):\n",
    "            self.filelist.append('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_%d_shuffled.h5'%i)\n",
    "        \n",
    "        self.train_split = 0.6 \n",
    "        self.test_split = 0.2 \n",
    "        self.validation_split = 0.2\n",
    "        self.fileindex = 0\n",
    "        self.filesize = filesize\n",
    "        self.position = 0\n",
    "    #function to call when generating data for training  \n",
    "    def train(self, cnn=False):\n",
    "        return self.batches(cnn)\n",
    "    #function to call when generating data for validation \n",
    "    def validation(self, cnn=False):\n",
    "        return self.batches(cnn)\n",
    "    #function to call when generating data for testing  \n",
    "    def test(self, cnn=False):\n",
    "        return self.batches(cnn)\n",
    "        \n",
    "    #The function which reads files to gather data until batch size is satisfied\n",
    "    def batch_helper(self, fileindex, position, batch_size):\n",
    "        '''\n",
    "        Yields batches of data of size N\n",
    "        '''\n",
    "        f = h5py.File(self.filelist[fileindex],'r')\n",
    "        print(self.filelist[fileindex],'first')\n",
    "        if (position + batch_size < self.filesize):\n",
    "            data = np.array(f['images'][position : position + batch_size])\n",
    "            target = np.array(f['target'][position : position + batch_size])\n",
    "            target = np.delete(target,0,1)\n",
    "\n",
    "            position += batch_size\n",
    "            f.close()\n",
    "            print('first position',position)\n",
    "            return data, target, fileindex, position\n",
    "        \n",
    "        else:\n",
    "            data = np.array(f['images'][position:])\n",
    "            target = np.array(f['target'][position:])\n",
    "            target = np.delete(target,0,1)\n",
    "            f.close()\n",
    "            \n",
    "            if (fileindex+1 < len(self.filelist)):\n",
    "                if(self.batch_size-data.shape[0]>0):\n",
    "                    while(self.batch_size-data.shape[0]>0):\n",
    "                        if(int(np.floor((self.batch_size-data.shape[0])/self.filesize))==0):\n",
    "                            number_of_files=1\n",
    "                        else:\n",
    "                            number_of_files=int(np.ceil((self.batch_size-data.shape[0])/self.filesize))\n",
    "                        for i in xrange(0,number_of_files):\n",
    "                            if(fileindex+i+2>len(self.filelist)):\n",
    "                                fileindex=0\n",
    "                                number_of_files=number_of_files-i\n",
    "                                i=0\n",
    "                            f = h5py.File(self.filelist[fileindex+i+1],'r')\n",
    "                            print(self.filelist[fileindex+i+1],'second')\n",
    "                            if (self.batch_size-data.shape[0]<self.filesize):\n",
    "                                position = self.batch_size-data.shape[0]\n",
    "                                data_ = np.array(f['images'][ : position])\n",
    "                                target_ = np.array(f['target'][:position])\n",
    "                                target_ = np.delete(target_,0,1)\n",
    "                            else:\n",
    "                                data_ = np.array(f['images'][:])\n",
    "                                target_ = np.array(f['target'])\n",
    "                                target_ = np.delete(target_,0,1)\n",
    "                            f.close()\n",
    "                    #data_, target_, fileindex, position = self.batch_helper(fileindex + 1, 0, batch_size - self.filesize+position)\n",
    "                            print( data.shape,data_.shape)\n",
    "                            print( target.shape,target_.shape)\n",
    "                            data = np.concatenate((data, data_), axis=0)\n",
    "                            target = np.concatenate((target, target_), axis=0)\n",
    "                    fileindex = fileindex +i+2\n",
    "                else:\n",
    "                    position = 0\n",
    "                    fileindex=fileindex+1\n",
    "            else:\n",
    "                fileindex = 0\n",
    "                position = 0\n",
    "            \n",
    "            return data, target, fileindex, position\n",
    "    #The function which loops indefinitely and continues to return data of the specified batch size\n",
    "    def batches(self, cnn):\n",
    "        while (self.fileindex < len(self.filelist)):\n",
    "            data, target, self.fileindex, self.position = self.batch_helper(self.fileindex, self.position, self.batch_size)\n",
    "            if data.shape[0]!=self.batch_size:\n",
    "                continue\n",
    "            if cnn==True:\n",
    "                data = np.swapaxes(data, 1, 3)\n",
    "                #data = np.swapaxes(data, 1, 2)\n",
    "                #data = np.swapaxes(data, 0, 1)\n",
    "                #data=data.reshape((data.shape[0],1,20,20,25))\n",
    "                \n",
    "            else:\n",
    "                data= np.reshape(data,(self.batch_size,-1))\n",
    "            yield (data, target/110.)\n",
    "        self.fileindex = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_1_shuffled.h5', 'first')\n",
      "Epoch 1/10\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_2_shuffled.h5', 'second')\n",
      "((10000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((10000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_2_shuffled.h5', 'second')\n",
      "((20000, 20, 20, 25), (5000, 20, 20, 25))\n",
      "((20000, 1), (5000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_3_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_4_shuffled.h5', 'second')\n",
      "((5000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((5000, 1), (10000, 1))\n",
      "25000/75000 [=========>....................] - ETA: 239s - loss: 0.0756('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_5_shuffled.h5', 'second')\n",
      "((15000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((15000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_6_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_7_shuffled.h5', 'second')\n",
      "((5000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((5000, 1), (10000, 1))\n",
      "50000/75000 [===================>..........] - ETA: 122s - loss: 0.0740('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_8_shuffled.h5', 'second')\n",
      "((15000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((15000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_9_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_10_shuffled.h5', 'second')\n",
      "((5000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((5000, 1), (10000, 1))\n",
      "75000/75000 [==============================] - 376s - loss: 0.0724     \n",
      "Epoch 2/10\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_2_shuffled.h5', 'second')\n",
      "((15000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((15000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_3_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_4_shuffled.h5', 'second')\n",
      "((5000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((5000, 1), (10000, 1))\n",
      "25000/75000 [=========>....................] - ETA: 19s - loss: 0.0685('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_5_shuffled.h5', 'second')\n",
      "((15000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((15000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_6_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_7_shuffled.h5', 'second')\n",
      "((5000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((5000, 1), (10000, 1))\n",
      "50000/75000 [===================>..........] - ETA: 10s - loss: 0.0673('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_8_shuffled.h5', 'second')\n",
      "((15000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((15000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_9_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_10_shuffled.h5', 'second')\n",
      "((5000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((5000, 1), (10000, 1))\n",
      "75000/75000 [==============================] - 30s - loss: 0.0661     \n",
      "Epoch 3/10\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_2_shuffled.h5', 'second')\n",
      "((15000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((15000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_3_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_4_shuffled.h5', 'second')\n",
      "((5000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((5000, 1), (10000, 1))\n",
      "25000/75000 [=========>....................] - ETA: 20s - loss: 0.0630('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_5_shuffled.h5', 'second')\n",
      "((15000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((15000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_6_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_7_shuffled.h5', 'second')\n",
      "((5000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((5000, 1), (10000, 1))\n",
      "50000/75000 [===================>..........] - ETA: 10s - loss: 0.0621('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_8_shuffled.h5', 'second')\n",
      "((15000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((15000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_9_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_10_shuffled.h5', 'second')\n",
      "((5000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((5000, 1), (10000, 1))\n",
      "75000/75000 [==============================] - 31s - loss: 0.0610     \n",
      "Epoch 4/10\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_2_shuffled.h5', 'second')\n",
      "((15000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((15000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_3_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_4_shuffled.h5', 'second')\n",
      "((5000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((5000, 1), (10000, 1))\n",
      "25000/75000 [=========>....................] - ETA: 21s - loss: 0.0584('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_5_shuffled.h5', 'second')\n",
      "((15000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((15000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_6_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_7_shuffled.h5', 'second')\n",
      "((5000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((5000, 1), (10000, 1))\n",
      "50000/75000 [===================>..........] - ETA: 10s - loss: 0.0575('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_8_shuffled.h5', 'second')\n",
      "((15000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((15000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_9_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_10_shuffled.h5', 'second')\n",
      "((5000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((5000, 1), (10000, 1))\n",
      "75000/75000 [==============================] - 32s - loss: 0.0566     \n",
      "Epoch 5/10\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_2_shuffled.h5', 'second')\n",
      "((15000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((15000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_3_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_4_shuffled.h5', 'second')\n",
      "((5000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((5000, 1), (10000, 1))\n",
      "25000/75000 [=========>....................] - ETA: 22s - loss: 0.0543('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_5_shuffled.h5', 'second')\n",
      "((15000, 20, 20, 25), (10000, 20, 20, 25))\n",
      "((15000, 1), (10000, 1))\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_6_shuffled.h5', 'first')\n",
      "('/data/shared/LCD/GammaEscan_shuffled_datasets/GammaEscan_7_shuffled.h5', 'second')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1b28cdff2210>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMy_Gen_Reg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m75000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, **kwargs)\u001b[0m\n\u001b[0;32m    649\u001b[0m                                         \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m                                         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m                                         max_q_size=max_q_size)\n\u001b[0m\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size)\u001b[0m\n\u001b[0;32m   1381\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1382\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1384\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1165\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1168\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.8.2-py2.7.egg/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    784\u001b[0m                         s.storage[0] = s.type.filter(\n\u001b[0;32m    785\u001b[0m                             \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m                             allow_downcast=s.allow_downcast)\n\u001b[0m\u001b[0;32m    787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.8.2-py2.7.egg/theano/tensor/type.pyc\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, data, strict, allow_downcast)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mallow_downcast\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Convert to self.dtype, regardless of the type of data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_asarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m                 \u001b[1;31m# TODO: consider to pad shape with ones to make it consistent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                 \u001b[1;31m# with self.broadcastable... like vector->row type thing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.8.2-py2.7.egg/theano/misc/safe_asarray.pyc\u001b[0m in \u001b[0;36m_asarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Convert into dtype object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Note that dtype comparison must be done by comparing their `num`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# attribute. One cannot assume that two identical data types are pointers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \"\"\"\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ds = My_Gen_Reg(25000, 10000)\n",
    "hist = dnn.fit_generator(ds.train(cnn=False), samples_per_epoch=75000, nb_epoch=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vs = My_Gen_E(20000, 10000)\n",
    "#early = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "hist = dnn.fit_generator(ds.train(cnn=False), samples_per_epoch=80000, nb_epoch=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savemodel(dnn,\"dnn_fixed\")\n",
    "%matplotlib inline\n",
    "show_losses([(\"mse\",hist)],\"dnn_fixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_file = open('dnn_fixed.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "dnn = model_from_json(loaded_model_json)\n",
    "dnn.compile(loss='mse', optimizer='sgd')\n",
    "# load weights into new model\n",
    "dnn.load_weights(\"dnn_fixed.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "md = []\n",
    "rmsd = []\n",
    "rel_error = []\n",
    "for i in xrange(10,110):\n",
    "    print (i)\n",
    "    if i==13:\n",
    "         continue\n",
    "    fn =('/data/kaustuv1993/EnergyScan_Gamma/GammaEscan_%dGeV_fulldataset.h5'%i)\n",
    "    f = h5py.File(fn,'r')\n",
    "    test_data = np.array(f['images'])\n",
    "    test_target=np.array(f['target'])\n",
    "    #test_data = np.swapaxes(test_data,1,3)\n",
    "    test_data= np.reshape(test_data,(10000,-1))\n",
    "    test_target = np.delete(test_target,0,1)\n",
    "    pred = dnn.predict(test_data)\n",
    "    mean = np.mean(pred*110- test_target)\n",
    "    diff = (np.mean((pred*110-test_target)**2))\n",
    "    rmsde = math.sqrt((diff))\n",
    "    re = np.mean(np.absolute((pred*110- test_target))/(test_target))\n",
    "    print (mean, rmsde, re)\n",
    "    md.append(mean)\n",
    "    rmsd.append(rmsde)\n",
    "    rel_error.append(re)\n",
    "\n",
    "plt.plot(md)\n",
    "plt.savefig('DNN Mean error per energy.pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rmsd)\n",
    "plt.savefig('DNN RMSD error per energy.pdf')\n",
    "plt.show() \n",
    "\n",
    "plt.plot(rel_error)\n",
    "plt.savefig('DNN Relative error per energy.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNNs with direct file feeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dnn2 = Sequential()\n",
    "dnn2.add(Dense(10000,input_shape=(10000,), activation='sigmoid'))\n",
    "dnn2.add(Dense(100, activation='sigmoid'))\n",
    "dnn2.add(Dropout(0.5))\n",
    "dnn2.add(Dense(10, activation='sigmoid'))\n",
    "dnn2.add(Dense(1, activation='linear'))\n",
    "#sgd=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=True)\n",
    "dnn2.compile(loss='mse', optimizer='sgd')\n",
    "#simple.load_weights('first_try.h5')\n",
    "dnn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def show_losses( histories,fname ):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    #plt.ylim(bottom=0)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Error by Epoch')\n",
    "    \n",
    "    colors=[]\n",
    "    do_acc=False\n",
    "    for label,loss in histories:\n",
    "        color = tuple(np.random.random(3))\n",
    "        colors.append(color)\n",
    "        l = label\n",
    "        vl= label+\" validation\"\n",
    "        if 'acc' in loss.history:\n",
    "            l+=' (acc %2.4f)'% (loss.history['acc'][-1])\n",
    "            do_acc = True\n",
    "        if 'val_acc' in loss.history:\n",
    "            vl+=' (acc %2.4f)'% (loss.history['val_acc'][-1])\n",
    "            do_acc = True\n",
    "        plt.plot(loss.history['loss'], label=l, color=color)\n",
    "        if 'val_loss' in loss.history:\n",
    "            plt.plot(loss.history['val_loss'], lw=2, ls='dashed', label=vl, color=color)\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.savefig('%s.pdf'%fname)\n",
    "    plt.show()\n",
    "    if not do_acc: \n",
    "\treturn\n",
    "\n",
    "    #plt.figure(figsize=(10,10))\n",
    "    #plt.xlabel('Epoch')\n",
    "    #plt.ylabel('Accuracy')\n",
    "    #for i,(label,loss) in enumerate(histories):\n",
    "    #    color = colors[i]\n",
    "    #    if 'acc' in loss.history:\n",
    "    #        plt.plot(loss.history['acc'], lw=2, label=label+\" accuracy\", color=color)\n",
    "    #    if 'val_acc' in loss.history:\n",
    "    #        plt.plot(loss.history['val_acc'], lw=2, ls='dashed', label=label+\" validation accuracy\", color=color)\n",
    "    #plt.legend(loc='lower right')\n",
    "    #plt.savefig('%s.png'%fname)\n",
    "   \n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for i in xrange(10,110):\n",
    "    if i==13:\n",
    "        continue\n",
    "    fn ='/data/shared/LCD/EnergyScan_Gamma_Shuffled/GammaEscan_%dGeV_fulldataset.h5'%i\n",
    "    f = h5py.File(fn,'r')\n",
    "    train_data = np.array(f['images'])\n",
    "    train_target = np.array(f['target'])[:,1]\n",
    "    #train_data = np.swapaxes(train_data, 1, 3)\n",
    "    train_data= np.reshape(train_data,(10000,-1))\n",
    "    print(i)\n",
    "    my_fit = dnn2.fit(train_data, train_target/110., nb_epoch=10, validation_split=0.2, batch_size=1000, verbose=1)\n",
    "    show_losses( [(\"mse\",my_fit)],\"dnn2_%d\"%i)\n",
    "    f.close()\n",
    "    fname = \"dnn2_file%d\"%i\n",
    "    loss = np.array(my_fit.history['loss'])\n",
    "    valoss = np.array(my_fit.history['val_loss'])\n",
    "    f = h5py.File(\"%s_losses.h5\"%fname,\"w\")\n",
    "    f.create_dataset('loss',data=loss)\n",
    "    f.create_dataset('val_loss',data=valoss)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "md = []\n",
    "rmsd = []\n",
    "rel_error = []\n",
    "for i in xrange(10,110):\n",
    "    print (i)\n",
    "    if i==13:\n",
    "         continue\n",
    "    fn =('/data/kaustuv1993/EnergyScan_Gamma/GammaEscan_%dGeV_fulldataset.h5'%i)\n",
    "    f = h5py.File(fn,'r')\n",
    "    test_data = np.array(f['images'])\n",
    "    test_target=np.array(f['target'])\n",
    "    #test_data = np.swapaxes(test_data,1,3)\n",
    "    test_data= np.reshape(test_data,(10000,-1))\n",
    "    test_target = np.delete(test_target,0,1)\n",
    "    pred = dnn.predict(test_data)\n",
    "    mean = np.mean(pred*110- test_target)\n",
    "    diff = (np.mean((pred*110-test_target)**2))\n",
    "    rmsde = math.sqrt((diff))\n",
    "    re = np.mean(np.absolute((pred*110- test_target))/(test_target))\n",
    "    print (mean, rmsde, re)\n",
    "    md.append(mean)\n",
    "    rmsd.append(rmsde)\n",
    "    rel_error.append(re)\n",
    "    f.close()\n",
    "\n",
    "plt.plot(md)\n",
    "plt.savefig('DNN2 Mean error per energy.pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rmsd)\n",
    "plt.savefig('DNN2 RMSD error per energy.pdf')\n",
    "plt.show() \n",
    "\n",
    "plt.plot(rel_error)\n",
    "plt.savefig('DNN2 Relative error per energy.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savemodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.arange(1,82)\n",
    "a = a.reshape(3,3,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.reshape(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in xrange(3):\n",
    "    print ('bla')\n",
    "print (j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
