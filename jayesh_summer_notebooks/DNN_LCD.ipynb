{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN not available)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "require(['codemirror/mode/clike/clike'], function(Clike) { console.log('ROOTaaS - C++ CodeMirror module loaded'); });"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.CodeCell.config_defaults.highlight_modes['magic_text/x-c++src'] = {'reg':[/^%%cpp/]};"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to ROOTaaS 6.06/04\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import glob\n",
    "from io_functions import *\n",
    "from nn_packages import  *\n",
    "from __future__ import print_function\n",
    "import h5py\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class to make generator returning both energy and label\n",
    "class My_Gen_EL:\n",
    "    def __init__( self, batch_size, filesize, filepattern='/data/shared/LCD/EnergyScan_Gamma_Shuffled/GammaEscan_*_*.h5'):\n",
    "        self.batch_size = batch_size\n",
    "        self.filelist = filter(None, os.popen('ls %s'%filepattern).read().split('\\n'))\n",
    "        #You can change the fraction of train, validation and test set here\n",
    "        self.train_split = 0.6 \n",
    "        self.test_split = 0.2 \n",
    "        self.validation_split = 0.2\n",
    "        self.fileindex = 0\n",
    "        self.filesize = filesize\n",
    "        self.position = 0\n",
    "    #function to call when generating data for training  \n",
    "    def train(self,cnn=False):\n",
    "        length = len(self.filelist)\n",
    "        #deleting the validation and test set filenames from the filelist\n",
    "        del self.filelist[np.floor((1-(self.train_split))*length).astype(int):]\n",
    "        return self.batches(cnn)\n",
    "    #function to call when generating data for testing\n",
    "    def test(self, cnn=False):\n",
    "        length = len(self.filelist)\n",
    "        #deleting the train and validation set filenames from the filelist\n",
    "        del self.filelist[:np.floor((1-self.test_split)*length).astype(int)+1]\n",
    "        return self.batches(cnn)\n",
    "    #function to call when generating data for validating\n",
    "    def validation(self, cnn=False):\n",
    "        length = len(self.filelist)\n",
    "        #modifying the filename list to only include files for validation set\n",
    "        self.filelist = self.filelist[np.floor(self.train_split*length+1).astype(int):np.floor((self.train_split+self.validation_split)*length+1).astype(int)]\n",
    "        return self.batches(cnn)\n",
    "    #The function which reads files and returns data of batch size of N\n",
    "    def batch_helper(self, fileindex, position, batch_size, train=True):\n",
    "        '''\n",
    "        Yields batches of data of size N\n",
    "        '''\n",
    "        f = h5py.File(self.filelist[fileindex],'r')\n",
    "        #If the data to be read can be read from the current file\n",
    "        if (position + batch_size < self.filesize):\n",
    "            data = np.array(f['images'][position : position + batch_size])\n",
    "            target = np.array(f['target'][position : position + batch_size])\n",
    "            #incrementing the position to start from while reading the next batch\n",
    "            position += batch_size\n",
    "            f.close()\n",
    "            \n",
    "            return data, target, fileindex, position\n",
    "        \n",
    "        else:\n",
    "        #if the data to be read exceeds the current file\n",
    "        #Read the data as much as we can from the current file\n",
    "            data = np.array(f['images'][position:])\n",
    "            target = np.array(f['target'][position:])\n",
    "            f.close()\n",
    "            #Read the remaining data from the next files by calling the same function recursively\n",
    "            #Also a safety check to see if the file opened is the last file\n",
    "            if (fileindex+1 < len(self.filelist)):\n",
    "                data_, target_, fileindex, position = self.batch_helper(fileindex + 1, 0, batch_size - self.filesize + position)\n",
    "                data = np.concatenate((data, data_), axis=0)\n",
    "                target = np.concatenate((target, target_), axis=0)\n",
    "            #if the file read is the last file, loop back to the beginning of the filname list\n",
    "            else:\n",
    "                fileindex = 0\n",
    "                position = 0\n",
    "            \n",
    "            return data, target, fileindex, position\n",
    "    #The function which loops indefinitely and continues to return data of the specified batch size\n",
    "    def batches(self, cnn):\n",
    "        #loop indefinetly\n",
    "        while (self.fileindex < len(self.filelist)):\n",
    "            data, target, self.fileindex, self.position = self.batch_helper(self.fileindex, self.position, self.batch_size)\n",
    "            if data.shape[0]!=self.batch_size:\n",
    "                continue\n",
    "            if cnn==True:\n",
    "                data = np.swapaxes(data,1,3)\n",
    "            else:\n",
    "                data= np.reshape(data,(self.batch_size,-1))\n",
    "            target0=target[:,0]\n",
    "            target1=target[:,1]\n",
    "            yield (data, [target[:,0], target[:,1]/110.])\n",
    "        self.fileindex = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class to make generator returning energy \n",
    "class My_Gen_E:\n",
    "    def __init__( self, batch_size, filesize, filepattern='/data/kaustuv1993/EnergyScan_Gamma/GammaEscan_*_shuffled.h5'):\n",
    "        self.batch_size = batch_size\n",
    "        self.filelist = filter(None, os.popen('ls %s'%filepattern).read().split('\\n'))\n",
    "        \n",
    "        self.train_split = 0.6 \n",
    "        self.test_split = 0.2 \n",
    "        self.validation_split = 0.2\n",
    "        self.fileindex = 0\n",
    "        self.filesize = filesize\n",
    "        self.position = 0\n",
    "    #function to call when generating data for training  \n",
    "    def train(self, cnn=False):\n",
    "        length = len(self.filelist)\n",
    "        del self.filelist[np.floor((1-(self.train_split))*length).astype(int):]\n",
    "        return self.batches(cnn)\n",
    "    #function to call when generating data for testing  \n",
    "    def test(self, cnn=False):\n",
    "        length = len(self.filelist)\n",
    "        del self.filelist[:np.floor((1-self.test_split)*length).astype(int)+1]\n",
    "        return self.batches(cnn)\n",
    "    #function to call when generating data for validation \n",
    "    def validation(self, cnn=False):\n",
    "        length = len(self.filelist)\n",
    "        self.filelist = self.filelist[np.floor(self.train_split*length+1).astype(int):np.floor((self.train_split+self.validation_split)*length+1).astype(int)]\n",
    "        return self.batches(cnn)\n",
    "        \n",
    "    #The function which reads files to gather data until batch size is satisfied\n",
    "    def batch_helper(self, fileindex, position, batch_size):\n",
    "        '''\n",
    "        Yields batches of data of size N\n",
    "        '''\n",
    "        f = h5py.File(self.filelist[fileindex],'r')\n",
    "        if (position + batch_size < self.filesize):\n",
    "            data = np.array(f['images'][position : position + batch_size])\n",
    "            target = np.array(f['target'][position : position + batch_size])\n",
    "            target = np.delete(target,0,1)\n",
    "\n",
    "            position += batch_size\n",
    "            f.close()\n",
    "            \n",
    "            return data, target, fileindex, position\n",
    "        \n",
    "        else:\n",
    "            data = np.array(f['images'][position:])\n",
    "            target = np.array(f['target'][position:])\n",
    "            target = np.delete(target,0,1)\n",
    "            f.close()\n",
    "            \n",
    "            if (fileindex+1 < len(self.filelist)):\n",
    "                data_, target_, fileindex, position = self.batch_helper(fileindex + 1, 0, batch_size - self.filesize + position)\n",
    "                data = np.concatenate((data, data_), axis=0)\n",
    "                target = np.concatenate((target, target_), axis=0)\n",
    "            \n",
    "            else:\n",
    "                fileindex = 0\n",
    "                position = 0\n",
    "            \n",
    "            return data, target, fileindex, position\n",
    "    #The function which loops indefinitely and continues to return data of the specified batch size\n",
    "    def batches(self, cnn):\n",
    "        while (self.fileindex < len(self.filelist)):\n",
    "            data, target, self.fileindex, self.position = self.batch_helper(self.fileindex, self.position, self.batch_size)\n",
    "            if data.shape[0]!=self.batch_size:\n",
    "                continue\n",
    "            if cnn==True:\n",
    "                data = np.swapaxes(data,1,3)\n",
    "            else:\n",
    "                data= np.reshape(data,(self.batch_size,-1))\n",
    "            yield (data, target/110.)\n",
    "        self.fileindex = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9962 100\n",
      "9961 100\n",
      "9979 100\n",
      "9975 99\n",
      "9952 99\n",
      "9976 92\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-099620ce89af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#count = 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(len(ds.filelist))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print('count is ',count)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mncols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3307ffd9cd5a>\u001b[0m in \u001b[0;36mbatches\u001b[1;34m(self, cnn)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileindex\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilelist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3307ffd9cd5a>\u001b[0m in \u001b[0;36mbatch_helper\u001b[1;34m(self, fileindex, position, batch_size)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'images'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/tmp/pip-build-vbVj8U/h5py/h5py/_objects.c:2579)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/tmp/pip-build-vbVj8U/h5py/h5py/_objects.c:2538)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/h5py/_hl/dataset.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m         \u001b[0mfspace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[1;31m# Patch up the output for NumPy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ds = My_Gen_E(10000, 10000)\n",
    "#vs = My_Gen_E(10000,10000)\n",
    "#count = 0\n",
    "#print(len(ds.filelist))\n",
    "for (data, target) in ds.train():\n",
    "    #print('count is ',count)\n",
    "    #code to check for unique rows in the data\n",
    "    ncols = data.shape[1]\n",
    "    dtype = data.dtype.descr * ncols\n",
    "    struct = data.view(dtype)\n",
    "    uniq = np.unique(struct)\n",
    "    uniq = uniq.view(data.dtype).reshape(-1, ncols)\n",
    "    #code to check for the number of unique rows (energy) in targets\n",
    "    count2 = np.unique(target, return_counts=True)[1]\n",
    "    print(uniq.shape[0],count2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple = Sequential()\n",
    "simple.add(Dense(1000, input_dim=(10000), activation='relu'))\n",
    "simple.add(Dense(100, activation='relu'))\n",
    "simple.add(Dense(1, activation='sigmoid'))\n",
    "simple.compile(loss='mae', optimizer='sgd')\n",
    "#simple.load_weights('first_try.h5')\n",
    "#simple.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(10, 4, 4, input_shape = (25, 20, 20), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist = simple.fit_generator(ds.train(), samples_per_epoch=10000, nb_epoch=5, validation_data= vs.validation(), nb_val_samples=5000,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = \"cnn2d_dense_4_mse_sgd_1\"\n",
    "cnn1.summary()\n",
    "cnn1.save_weights('%s.h5'%model_name, overwrite=True)\n",
    "model_json = cnn1.to_json()\n",
    "with open(\"%s.json\"%model_name, \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple.save_weights('conv_relu10000relu100sigmoid1epoch50adam_mae_mse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_file = open('/home/kaustuv1993/Notebooks/models/cnn2d_dense_4_mse_sgd_300epoch.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.compile(loss='mse', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('/home/kaustuv1993/Notebooks/weights/cnn2d_dense_4_mse_sgd_300epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_losses( histories, fname ):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    #plt.ylim(bottom=0)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Error by Epoch')\n",
    "    \n",
    "    colors=[]\n",
    "    do_acc=False\n",
    "    for label,loss in histories:\n",
    "        color = tuple(np.random.random(3))\n",
    "        colors.append(color)\n",
    "        l = label\n",
    "        vl= label+\" validation\"\n",
    "        if 'acc' in loss.history:\n",
    "            l+=' (acc %2.4f)'% (loss.history['acc'][-1])\n",
    "            do_acc = True\n",
    "        if 'val_acc' in loss.history:\n",
    "            vl+=' (acc %2.4f)'% (loss.history['val_acc'][-1])\n",
    "            do_acc = True\n",
    "        plt.plot(loss.history['loss'], label=l, color=color)\n",
    "        if 'val_loss' in loss.history:\n",
    "            plt.plot(loss.history['val_loss'], lw=2, ls='dashed', label=vl, color=color)\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    plt.savefig('%s.png'%fname)\n",
    "    plt.savefig('%s.pdf'%fname)\n",
    "    if not do_acc: \n",
    "\treturn\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    for i,(label,loss) in enumerate(histories):\n",
    "        color = colors[i]\n",
    "        if 'acc' in loss.history:\n",
    "            plt.plot(loss.history['acc'], lw=2, label=label+\" accuracy\", color=color)\n",
    "        if 'val_acc' in loss.history:\n",
    "            plt.plot(loss.history['val_acc'], lw=2, ls='dashed', label=label+\" validation accuracy\", color=color)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_losses([(\"mse\",hist)],'not_important')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_per_energy():\n",
    "    average_mse = []\n",
    "    for i in xrange(10,110):\n",
    "        print (i)\n",
    "        fn =('/data/kaustuv1993/EnergyScan_Gamma/GammaEscan_%d_shuffled.h5'%i)\n",
    "        f = h5py.File(fn,'r')\n",
    "        test_data = np.array(f['images'])\n",
    "        test_target=np.array(f['target'])\n",
    "        test_data.shape\n",
    "        test_data = np.swapaxes(test_data,1,3)\n",
    "        test_target = np.delete(test_target,0,1)\n",
    "        test_target.shape\n",
    "        \n",
    "        #pred = model.predict(test_data)\n",
    "        #print([test_target])\n",
    "        #plt.hist( np.ravel(pred[1]*110)/test_target[:] , label='energy resolution', bins=40)\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "        #print(pred)\n",
    "        #average_mse.append(pred)\n",
    "    #print (average_mse)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_per_energy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unique, counts = np.unique(test_target[:,1], return_counts=True)\n",
    "        #print (fn, unique, counts)\n",
    "        #print(fn)\n",
    "        #print(test_target[:,1].T)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#functional Keras model with one hidden layer\n",
    "#from keras.models import Model\n",
    "#from keras.layers import Input, Dense, Dropout\n",
    "#i = Input(shape=(10000,))\n",
    "#d = Dense(1000, activation='sigmoid')(i)\n",
    "#dp = Dropout(0.5)(d)\n",
    "#one output layer with two output neurons, one for classification and another for regression\n",
    "#oc = Dense(1,activation='sigmoid', name='label')(d)\n",
    "#oe = Dense(1,activation='sigmoid', name='energy')(d)\n",
    "\n",
    "#m_model = Model(input=i, output=[oc,oe])\n",
    "#m_model.compile(loss=['binary_crossentropy','mse'], optimizer='sgd')\n",
    "#Getting the data and training the net\n",
    "#hist = m_model.fit_generator(ds.batches(), samples_per_epoch=1800000, nb_epoch = 2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = simple.evaluate_generator(ds.test(), val_samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([1,1,1,2,2,2,5,25,1,1])\n",
    "y = np.bincount(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1,1,1,2,2,2,5,25,1,1])\n",
    "unique, counts = np.unique(x, return_counts=True)\n",
    "print (x)\n",
    "print (unique, counts)\n",
    "print (np.argmin(counts), counts, counts.min())\n",
    "current = np.split(x,np.cumsum(counts))\n",
    "current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.arange(10,120,10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_per_energy(simple):\n",
    "    average_mse = []\n",
    "    for i in xrange(10,110):\n",
    "        print (i)\n",
    "        fn =('/data/kaustuv1993/EnergyScan_Gamma/GammaEscan_%dGeV_fulldataset.h5'%i)\n",
    "        f = h5py.File(fn,'r')\n",
    "        test_data = np.array(f['images'])\n",
    "        test_target=np.array(f['target'])\n",
    "        #test_data = np.swapaxes(test_data,1,3)\n",
    "        test_data = test_data.reshape(test_data.shape[0],-1)\n",
    "        test_target = np.delete(test_target,0,1)\n",
    "        pred = simple.predict(test_data)\n",
    "        print (pred[:6], test_target[:6]/110.)\n",
    "        #print([test_target])\n",
    "        #plt.hist( test_target - (np.ravel(pred[1]*110))/test_target[:] , label='energy resolution', bins=40)\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "        #print(pred)\n",
    "        #average_mse.append(pred)\n",
    "    #print (average_mse)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_per_energy(simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rperm=np.random.permutation(3)\n",
    "print (rperm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]],\n",
       "\n",
       "       [[16, 17, 18, 19],\n",
       "        [20, 21, 22, 23],\n",
       "        [24, 25, 26, 27],\n",
       "        [28, 29, 30, 31]],\n",
       "\n",
       "       [[32, 33, 34, 35],\n",
       "        [36, 37, 38, 39],\n",
       "        [40, 41, 42, 43],\n",
       "        [44, 45, 46, 47]],\n",
       "\n",
       "       [[48, 49, 50, 51],\n",
       "        [52, 53, 54, 55],\n",
       "        [56, 57, 58, 59],\n",
       "        [60, 61, 62, 63]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(64)\n",
    "a = a.reshape((4,4,4))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[32, 33, 34, 35],\n",
       "        [36, 37, 38, 39],\n",
       "        [40, 41, 42, 43],\n",
       "        [44, 45, 46, 47]],\n",
       "\n",
       "       [[48, 49, 50, 51],\n",
       "        [52, 53, 54, 55],\n",
       "        [56, 57, 58, 59],\n",
       "        [60, 61, 62, 63]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/shared/LCD/EnergyScan_Gamma_Shuffled/GammaEscan_99GeV_fulldataset.h5\n",
      "(10000, 20, 20, 25) 85\n"
     ]
    }
   ],
   "source": [
    "filepattern='/data/shared/LCD/EnergyScan_Gamma_Shuffled/GammaEscan_*GeV_fulldataset.h5'\n",
    "filelist = filter(None, os.popen('ls %s'%filepattern).read().split('\\n'))\n",
    "print (filelist[98])\n",
    "f = h5py.File(filelist[98],'r')\n",
    "data = np.array(f['images'][0 : 10000])\n",
    "target = np.array(f['target'][0 : 10000])\n",
    "target = np.delete(target,0,1)\n",
    "u = np.unique(target)\n",
    "print(data.shape, u.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
